# -*- coding: utf-8 -*-
"""sarsa vs q-learning for taxi

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12MkEpUwX11oJAyAAeYqV_cOtCxxJPwNw

Taxi_SARSA
"""

import numpy as np
import gym
import matplotlib.pyplot as plt
from tqdm import tqdm

env = gym.make('Taxi-v3')


def action_epsilon_greedy(q, s, epsilon=0.1):
    if np.random.rand() > epsilon:
        return greedy(q, s)
    return np.random.randint(env.action_space.n)

def greedy(q, s):
    return np.argmax(q[s])


def evaluate_policy(q, n=5000):
    tot_returns = 0
    for x in range(n):
        done = False
        s = env.reset()

        while not done :
            a = action_epsilon_greedy(q, s, epsilon=0.0)
            s, reward, done, _ = env.step(a)
            tot_returns += reward

    return tot_returns / n

def sarsa(alpha=0.1, gamma=0.99, epsilon=0.1, q=None, env=env):
    if q is None:
        q = np.zeros((env.observation_space.n, env.action_space.n)).astype(np.float32)

    nb_episodes = 10000
    steps = 200
    progress = []

    for i in tqdm(range(nb_episodes)):
        done = False
        s = env.reset()
        a = action_epsilon_greedy(q, s, epsilon=epsilon)

        while not done :
            new_state, reward, done, _ = env.step(a)
            new_action = action_epsilon_greedy(q, new_state, epsilon=epsilon)
            q[s, a] = q[s, a] + alpha * (reward + gamma * q[new_state, new_action] - q[s, a])
            s = new_state
            a = new_action

        if i % steps == 0:
            average_return = evaluate_policy(q, n=5000)
            progress.append(evaluate_policy(q, n=5000))
            print("Episode : {}, Average_Return : {}". format(i, average_return))

    return q, progress

def plot_progress(progress):
    x_values = list(range(0, len(progress) * 200, 200))
    plt.plot(x_values, progress)
    plt.title('Performance Over Episodes(SARSA)')
    plt.xlabel('Episodes')
    plt.ylabel('Average Return')
    plt.show()

q, progress = sarsa(alpha=0.1, epsilon=0.1, gamma=0.99)
print("Evaluate_Average_Return : {}". format(evaluate_policy(q, n=10000)))
plot_progress(progress)

"""Taxi_Q-LEARNING"""

import numpy as np
import gym
import matplotlib.pyplot as plt
from tqdm import tqdm

env = gym.make('Taxi-v3')

def action_epsilon_greedy(q, s, epsilon=0.0):
    if np.random.rand() > epsilon:
        return greedy(q, s)
    return np.random.randint(env.action_space.n)

def greedy(q, s):
    return np.argmax(q[s])

def evaluate_policy(q, n=5000):
    tot_returns = 0
    for x in range(n):
        done = False
        s = env.reset()


        while not done :
            a = action_epsilon_greedy(q, s, epsilon=0.0)
            s, reward, done, _ = env.step(a)
            tot_returns += reward


    return tot_returns / n

def q_learning(alpha=0.1, gamma=0.99, epsilon=0.1, q=None, env=env):
    if q is None:
        q = np.zeros((env.observation_space.n, env.action_space.n)).astype(np.float32)

    nb_episodes = 10000
    steps = 200
    progress = []

    for i in tqdm(range(nb_episodes)):
        done = False
        s = env.reset()

        while not done:
            a = action_epsilon_greedy(q, s, epsilon=epsilon)
            new_state, reward, done, _ = env.step(a)
            # Q-learning update rule
            q[s, a] = q[s, a] + alpha * (reward + gamma * np.max(q[new_state, :]) - q[s, a])
            s = new_state


        if i % steps == 0:
            average_return = evaluate_policy(q, n=5000)
            progress.append(evaluate_policy(q, n=5000))
            print("Episode : {}, Average_Return : {}". format(i, average_return))

    return q, progress

def plot_progress(progress):
    x_values = list(range(0, len(progress) * 200, 200))
    plt.plot(x_values, progress)
    plt.title('Performance Over Episodes(Q-Learning)')
    plt.xlabel('Episodes')
    plt.ylabel('Average Return')
    plt.show()

q, progress = q_learning(alpha=0.1, epsilon=0.1, gamma=0.99)
print("Evaluate_Average_Return : {}". format(evaluate_policy(q, n=10000)))
plot_progress(progress)